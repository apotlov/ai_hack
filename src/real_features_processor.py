"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ —Å–∏—Å—Ç–µ–º—ã
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
import logging
from datetime import datetime, timedelta
import re

from real_data_loader import RealDataLoader
from audio_processor import AudioProcessor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RealFeaturesProcessor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ —Å–∏—Å—Ç–µ–º—ã
    """

    def __init__(self, data_dir: str = "data"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

        Args:
            data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        self.data_dir = data_dir
        self.data_loader = RealDataLoader(data_dir)
        self.audio_processor = AudioProcessor()

    def extract_amplitude_features(self, amplitude_data: pd.DataFrame) -> pd.DataFrame:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ amplitude –¥–∞–Ω–Ω—ã—Ö (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–≤–æ–Ω–∫–æ–≤)

        Args:
            amplitude_data: DataFrame —Å amplitude –¥–∞–Ω–Ω—ã–º–∏

        Returns:
            DataFrame —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        logger.info("üîß –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ amplitude –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        if amplitude_data.empty:
            return pd.DataFrame()

        features_list = []

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ session_id –∏–ª–∏ –¥—Ä—É–≥–æ–º—É –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É
        group_col = self._find_group_column(amplitude_data)
        if not group_col:
            logger.warning("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ amplitude –¥–∞–Ω–Ω—ã—Ö")
            return pd.DataFrame()

        logger.info(f"üìä –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–æ–Ω–∫–µ: {group_col}")

        for group_id, group_data in amplitude_data.groupby(group_col):
            try:
                features = self._extract_group_amplitude_features(group_id, group_data)
                features_list.append(features)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥—Ä—É–ø–ø—ã {group_id}: {e}")
                continue

        if not features_list:
            return pd.DataFrame()

        features_df = pd.DataFrame(features_list)
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ amplitude –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_df.shape}")

        return features_df

    def _find_group_column(self, data: pd.DataFrame) -> Optional[str]:
        """
        –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        """
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        primary_cols = ['APPLICATIONID', 'applicationid']
        secondary_cols = ['session_id', 'ID', 'id', 'call_id', 'user_id', 'client_id']

        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º APPLICATIONID
        for col in primary_cols:
            if col in data.columns:
                return col

        # –ó–∞—Ç–µ–º –∏—â–µ–º —Å—Ä–µ–¥–∏ –≤—Ç–æ—Ä–∏—á–Ω—ã—Ö
        for col in secondary_cols:
            if col in data.columns:
                return col

        return None

    def _extract_group_amplitude_features(self, group_id: str, group_data: pd.DataFrame) -> Dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã amplitude –¥–∞–Ω–Ω—ã—Ö
        """
        features = {
            'applicationid': group_id,
            'amplitude_records_count': len(group_data)
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
        numeric_cols = group_data.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            if group_data[col].notna().sum() > 0:
                values = group_data[col].dropna()

                features.update({
                    f'amplitude_{col}_mean': values.mean(),
                    f'amplitude_{col}_std': values.std(),
                    f'amplitude_{col}_min': values.min(),
                    f'amplitude_{col}_max': values.max(),
                    f'amplitude_{col}_median': values.median(),
                    f'amplitude_{col}_q25': values.quantile(0.25),
                    f'amplitude_{col}_q75': values.quantile(0.75),
                    f'amplitude_{col}_skew': values.skew(),
                    f'amplitude_{col}_kurt': values.kurtosis()
                })

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞
        time_cols = [col for col in group_data.columns if 'time' in col.lower() or 'date' in col.lower()]

        for time_col in time_cols:
            if group_data[time_col].notna().sum() > 1:
                try:
                    times = pd.to_datetime(group_data[time_col]).dropna()
                    if len(times) > 1:
                        time_diffs = times.diff().dt.total_seconds().dropna()

                        features.update({
                            f'amplitude_{time_col}_span_seconds': (times.max() - times.min()).total_seconds(),
                            f'amplitude_{time_col}_avg_interval': time_diffs.mean(),
                            f'amplitude_{time_col}_std_interval': time_diffs.std()
                        })
                except:
                    pass

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        categorical_cols = group_data.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            if group_data[col].notna().sum() > 0:
                value_counts = group_data[col].value_counts()

                features.update({
                    f'amplitude_{col}_unique_count': len(value_counts),
                    f'amplitude_{col}_most_common': value_counts.index[0] if len(value_counts) > 0 else 'unknown',
                    f'amplitude_{col}_most_common_ratio': value_counts.iloc[0] / len(group_data) if len(value_counts) > 0 else 0
                })

        return features

    def extract_app_features(self, app_data: pd.DataFrame) -> pd.DataFrame:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –∑–∞—è–≤–æ–∫

        Args:
            app_data: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞—è–≤–æ–∫

        Returns:
            DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∑–∞—è–≤–æ–∫
        """
        logger.info("üì± –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞—è–≤–æ–∫...")

        if app_data.empty:
            return pd.DataFrame()

        features_list = []
        group_col = self._find_group_column(app_data)

        if not group_col:
            # –ï—Å–ª–∏ –Ω–µ—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–æ—á–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ
            for idx, row in app_data.iterrows():
                features = self._extract_single_app_features(f"app_{idx}", row)
                features_list.append(features)
        else:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É
            for group_id, group_data in app_data.groupby(group_col):
                # –î–ª—è –∑–∞—è–≤–æ–∫ –æ–±—ã—á–Ω–æ –æ–¥–Ω–∞ –∑–∞–ø–∏—Å—å –Ω–∞ –≥—Ä—É–ø–ø—É, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ
                features = self._extract_group_app_features(group_id, group_data)
                features_list.append(features)

        if not features_list:
            return pd.DataFrame()

        features_df = pd.DataFrame(features_list)
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞—è–≤–æ–∫: {features_df.shape}")

        return features_df

    def _extract_single_app_features(self, row_id: str, row: pd.Series) -> Dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∑–∞—è–≤–∫–∏
        """
        features = {'applicationid': row_id}

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        for col, value in row.items():
            if pd.isna(value):
                continue

            if isinstance(value, (int, float)):
                features[f'app_{col}'] = value
            elif isinstance(value, str):
                # –î–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏
                features[f'app_{col}_length'] = len(value)

                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ñ—Ä
                features[f'app_{col}_digit_count'] = sum(c.isdigit() for c in value)

                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É–∫–≤
                features[f'app_{col}_alpha_count'] = sum(c.isalpha() for c in value)

                # –ï—Å—Ç—å –ª–∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
                features[f'app_{col}_has_special'] = int(bool(re.search(r'[^a-zA-Z0-9–∞-—è–ê-–Ø\s]', value)))

        return features

    def _extract_group_app_features(self, group_id: str, group_data: pd.DataFrame) -> Dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø—ã –∑–∞—è–≤–æ–∫
        """
        features = {
            'applicationid': group_id,
            'app_records_count': len(group_data)
        }

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        numeric_cols = group_data.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            if group_data[col].notna().sum() > 0:
                values = group_data[col].dropna()

                features.update({
                    f'app_{col}_mean': values.mean(),
                    f'app_{col}_sum': values.sum(),
                    f'app_{col}_max': values.max(),
                    f'app_{col}_min': values.min()
                })

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        categorical_cols = group_data.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            if group_data[col].notna().sum() > 0:
                unique_values = group_data[col].nunique()
                features[f'app_{col}_unique_count'] = unique_values

        return features

    def extract_audio_features(self, audio_metadata: pd.DataFrame) -> pd.DataFrame:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö

        Args:
            audio_metadata: DataFrame —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤

        Returns:
            DataFrame —Å –∞—É–¥–∏–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        logger.info("üéµ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        if audio_metadata.empty:
            return pd.DataFrame()

        features_list = []

        for _, row in audio_metadata.iterrows():
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                metadata_features = self._extract_audio_metadata_features(row)

                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∏–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ –ø—Ä–∏–∑–Ω–∞–∫–∏
                if 'file_path' in row and pd.notna(row['file_path']):
                    audio_features = self.audio_processor.extract_audio_features(row['file_path'])

                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∞—É–¥–∏–æ –ø—Ä–∏–∑–Ω–∞–∫–∏
                    combined_features = {**metadata_features, **audio_features}
                else:
                    combined_features = metadata_features

                features_list.append(combined_features)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ {row.get('session_id', 'unknown')}: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                features_list.append(self._get_default_audio_features(row))

        if not features_list:
            return pd.DataFrame()

        features_df = pd.DataFrame(features_list)

        # –î–æ–±–∞–≤–ª—è–µ–º applicationid –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if 'applicationid' not in features_df.columns:
            features_df['applicationid'] = [f'audio_{i}' for i in range(len(features_df))]

        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∞—É–¥–∏–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_df.shape}")

        return features_df

    def _extract_audio_metadata_features(self, audio_row: pd.Series) -> Dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞
        """
        features = {}

        # APPLICATIONID (–æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª—é—á —Å–≤—è–∑–∏)
        if 'applicationid' in audio_row and pd.notna(audio_row['applicationid']):
            features['applicationid'] = audio_row['applicationid']
        elif 'call_id' in audio_row and pd.notna(audio_row['call_id']):
            # Fallback –Ω–∞ call_id –µ—Å–ª–∏ –Ω–µ—Ç applicationid
            features['applicationid'] = audio_row['call_id']

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'datetime' in audio_row and pd.notna(audio_row['datetime']):
            call_time = audio_row['datetime']
            features.update({
                'audio_call_hour': call_time.hour,
                'audio_call_weekday': call_time.weekday(),
                'audio_call_is_weekend': int(call_time.weekday() >= 5),
                'audio_call_is_night': int(call_time.hour >= 22 or call_time.hour <= 6),
                'audio_call_is_business_hours': int(9 <= call_time.hour <= 17)
            })

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–∞
        if 'phone' in audio_row and pd.notna(audio_row['phone']):
            phone = str(audio_row['phone'])
            features.update({
                'audio_phone_length': len(phone),
                'audio_phone_starts_with_7': int(phone.startswith('7')),
                'audio_phone_starts_with_8': int(phone.startswith('8')),
                'audio_phone_has_country_code': int(len(phone) > 10)
            })

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ–¥–æ–≤
        if 'codes' in audio_row and isinstance(audio_row['codes'], list):
            codes = audio_row['codes']
            features.update({
                'audio_codes_count': len(codes),
                'audio_has_codes': int(len(codes) > 0)
            })

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–¥–æ–≤ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            for i, code in enumerate(codes[:3]):
                features[f'audio_code_{i+1}'] = code

        # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        if 'file_size' in audio_row and pd.notna(audio_row['file_size']):
            file_size = audio_row['file_size']
            features.update({
                'audio_file_size_bytes': file_size,
                'audio_file_size_mb': file_size / (1024 * 1024),
                'audio_estimated_duration': file_size / 32000  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            })

        return features

    def _get_default_audio_features(self, audio_row: pd.Series) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ
        """
        features = {
            'applicationid': audio_row.get('applicationid', audio_row.get('call_id', 'unknown')),
            'audio_processing_error': 1
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞—É–¥–∏–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        audio_feature_names = self.audio_processor.get_feature_names()
        for feature_name in audio_feature_names:
            if feature_name not in ['file_name', 'sample_rate']:
                features[feature_name] = 0.0

        return features

    def extract_temporal_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

        Args:
            data: DataFrame —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

        Returns:
            DataFrame —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        logger.info("‚è∞ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        if data.empty:
            return pd.DataFrame()

        features_list = []
        group_col = self._find_group_column(data)

        if not group_col:
            return pd.DataFrame()

        for group_id, group_data in data.groupby(group_col):
            features = {'applicationid': group_id}

            # –ò—â–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            time_columns = []
            for col in group_data.columns:
                if any(keyword in col.lower() for keyword in ['time', 'date', 'datetime']):
                    time_columns.append(col)

            for time_col in time_columns:
                try:
                    times = pd.to_datetime(group_data[time_col]).dropna()

                    if len(times) > 0:
                        # –û—Å–Ω–æ–≤–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        features.update({
                            f'temporal_{time_col}_count': len(times),
                            f'temporal_{time_col}_span_minutes': (times.max() - times.min()).total_seconds() / 60,
                            f'temporal_{time_col}_first_hour': times.min().hour,
                            f'temporal_{time_col}_last_hour': times.max().hour
                        })

                        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —á–∞—Å–∞–º
                        hour_counts = times.dt.hour.value_counts()
                        features.update({
                            f'temporal_{time_col}_unique_hours': len(hour_counts),
                            f'temporal_{time_col}_night_ratio': ((times.dt.hour >= 22) | (times.dt.hour <= 6)).mean(),
                            f'temporal_{time_col}_business_hours_ratio': ((times.dt.hour >= 9) & (times.dt.hour <= 17)).mean()
                        })

                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ {time_col}: {e}")
                    continue

            features_list.append(features)

        if not features_list:
            return pd.DataFrame()

        features_df = pd.DataFrame(features_list)
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_df.shape}")

        return features_df

    def combine_all_features(self) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –µ–¥–∏–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç

        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–ø—Ä–∏–∑–Ω–∞–∫–∏, —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏)
        """
        logger.info("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        amplitude_data = self.data_loader.load_amplitude_chunks()
        app_data = self.data_loader.load_app_data()
        target_data = self.data_loader.load_target_data()
        audio_metadata = self.data_loader.get_audio_files_metadata()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        amplitude_features = self.extract_amplitude_features(amplitude_data)
        app_features = self.extract_app_features(app_data)
        audio_features = self.extract_audio_features(audio_metadata)
        temporal_features = self.extract_temporal_features(amplitude_data)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        all_features = [amplitude_features, app_features, audio_features, temporal_features]

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã
        non_empty_features = [df for df in all_features if not df.empty]

        if not non_empty_features:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞")
            return pd.DataFrame(), pd.Series()

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ applicationid
        combined_features = non_empty_features[0]

        for features_df in non_empty_features[1:]:
            if 'applicationid' in features_df.columns and 'applicationid' in combined_features.columns:
                combined_features = pd.merge(
                    combined_features, features_df,
                    on='applicationid',
                    how='outer',
                    suffixes=('', '_dup')
                )

                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–æ–ª–æ–Ω–∫–∏
                dup_cols = [col for col in combined_features.columns if col.endswith('_dup')]
                combined_features = combined_features.drop(columns=dup_cols)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ü–µ–ª–µ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        target_series = pd.Series(dtype=int)

        if not target_data.empty:
            target_col = self._find_target_column(target_data)
            merge_col = self._find_group_column(target_data)

            logger.info(f"üîç –¶–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞: {target_col}")
            logger.info(f"üîç –ö–æ–ª–æ–Ω–∫–∞ –¥–ª—è —Å–ª–∏—è–Ω–∏—è: {merge_col}")
            logger.info(f"üîç –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ target_data: {list(target_data.columns)}")
            logger.info(f"üîç –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ combined_features: {list(combined_features.columns)}")

            if 'applicationid' in combined_features.columns:
                logger.info(f"üîç –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö applicationid –≤ features: {combined_features['applicationid'].nunique()}")
                logger.info(f"üîç –ü—Ä–∏–º–µ—Ä—ã applicationid –≤ features: {combined_features['applicationid'].head().tolist()}")

            if merge_col and merge_col in target_data.columns:
                logger.info(f"üîç –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö {merge_col} –≤ target_data: {target_data[merge_col].nunique()}")
                logger.info(f"üîç –ü—Ä–∏–º–µ—Ä—ã {merge_col} –≤ target_data: {target_data[merge_col].head().tolist()}")

            if target_col and merge_col and merge_col in combined_features.columns:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–π –ø–µ—Ä–µ–¥ —Å–ª–∏—è–Ω–∏–µ–º
                features_keys = set(combined_features['applicationid'].astype(str))
                target_keys = set(target_data[merge_col].astype(str))
                intersection = features_keys.intersection(target_keys)

                logger.info(f"üîç –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–π: {len(intersection)} –∏–∑ {len(features_keys)} features –∏ {len(target_keys)} target")

                if len(intersection) == 0:
                    logger.error("‚ùå –ù–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª—é—á–∞–º–∏ features –∏ target –¥–∞–Ω–Ω—ã—Ö!")
                    logger.info(f"üîç –ü–µ—Ä–≤—ã–µ 5 –∫–ª—é—á–µ–π features: {list(features_keys)[:5]}")
                    logger.info(f"üîç –ü–µ—Ä–≤—ã–µ 5 –∫–ª—é—á–µ–π target: {list(target_keys)[:5]}")

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ü–µ–ª–µ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ APPLICATIONID
                final_data = pd.merge(
                    combined_features, target_data,
                    left_on='applicationid', right_on=merge_col,
                    how='inner'
                )

                logger.info(f"üîç –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è: {final_data.shape}")

                if not final_data.empty:
                    target_series = final_data[target_col]

                    # –£–¥–∞–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –∫–æ–ª–æ–Ω–∫—É –∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    cols_to_drop = [target_col, merge_col] + [col for col in final_data.columns if col.endswith('_y')]
                    final_data = final_data.drop(columns=cols_to_drop, errors='ignore')

                    combined_features = final_data
                else:
                    logger.error("‚ùå –ü–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è –ø–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º!")
            else:
                logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–ª–∏—è–Ω–∏—è: target_col={target_col}, merge_col={merge_col}")
                if merge_col and merge_col not in combined_features.columns:
                    logger.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ {merge_col} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ combined_features")
        else:
            logger.error("‚ùå Target data –ø—É—Å—Ç!")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        combined_features = self._clean_features(combined_features)

        logger.info(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {combined_features.shape}")
        logger.info(f"üéØ –¶–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫: {len(target_series)}")

        return combined_features, target_series

    def _find_target_column(self, target_data: pd.DataFrame) -> Optional[str]:
        """
        –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        """
        possible_target_cols = [
            'is_fraud', 'target', 'fraud', 'label', 'class',
            '–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '–º–µ—Ç–∫–∞', 'fraud_flag'
        ]

        for col in possible_target_cols:
            if col in target_data.columns:
                return col

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∏—Å–ª–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É
        numeric_cols = target_data.select_dtypes(include=[np.number]).columns
        return numeric_cols[-1] if len(numeric_cols) > 0 else None

    def _clean_features(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """
        –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        if features_df.empty:
            return features_df

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–æ–ª–æ–Ω–∫–∏
        features_df = features_df.loc[:, ~features_df.columns.duplicated()]

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        features_df[numeric_cols] = features_df[numeric_cols].fillna(0)

        categorical_cols = features_df.select_dtypes(include=['object']).columns
        features_df[categorical_cols] = features_df[categorical_cols].fillna('unknown')

        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        constant_cols = features_df.columns[features_df.nunique() <= 1]
        if len(constant_cols) > 0:
            logger.info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(constant_cols)}")
            features_df = features_df.drop(columns=constant_cols)

        # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        service_cols = [col for col in features_df.columns if
                       col.startswith('applicationid') or col == 'applicationid']

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É applicationid –∫–æ–ª–æ–Ω–∫—É, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —É–¥–∞–ª—è–µ–º
        if 'applicationid' in features_df.columns:
            other_app_cols = [col for col in service_cols if col != 'applicationid']
            if other_app_cols:
                features_df = features_df.drop(columns=other_app_cols)

        logger.info(f"üßπ –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ—á–∏—â–µ–Ω—ã: {features_df.shape}")

        return features_df

    def create_prediction_features(self, data_dir: str) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–±–µ–∑ —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫)

        Args:
            data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

        Returns:
            DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        """
        logger.info("üîÆ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")

        # –í—Ä–µ–º–µ–Ω–Ω–æ –º–µ–Ω—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–∞–Ω–Ω—ã—Ö
        original_data_dir = self.data_dir
        self.data_loader = RealDataLoader(data_dir)

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–±–µ–∑ target_data)
            amplitude_data = self.data_loader.load_amplitude_chunks()
            app_data = self.data_loader.load_app_data()
            audio_metadata = self.data_loader.get_audio_files_metadata()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            amplitude_features = self.extract_amplitude_features(amplitude_data)
            app_features = self.extract_app_features(app_data)
            audio_features = self.extract_audio_features(audio_metadata)
            temporal_features = self.extract_temporal_features(amplitude_data)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            all_features = [amplitude_features, app_features, audio_features, temporal_features]
            non_empty_features = [df for df in all_features if not df.empty]

            if not non_empty_features:
                return pd.DataFrame()

            combined_features = non_empty_features[0]

            for features_df in non_empty_features[1:]:
                if 'applicationid' in features_df.columns and 'applicationid' in combined_features.columns:
                    combined_features = pd.merge(
                        combined_features, features_df,
                        on='applicationid',
                        how='outer',
                        suffixes=('', '_dup')
                    )

            # –û—á–∏—â–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            combined_features = self._clean_features(combined_features)

            logger.info(f"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≥–æ—Ç–æ–≤—ã: {combined_features.shape}")

            return combined_features

        finally:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            self.data_loader = RealDataLoader(original_data_dir)
