"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ —Å–∏—Å—Ç–µ–º—ã
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
import re
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RealDataLoader:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ —Å–∏—Å—Ç–µ–º—ã

    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:
    - train_amplitude_chunk_XX.parquet - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∑–≤–æ–Ω–∫–æ–≤ (—á–∞–Ω–∫–∏)
    - train_app_data.parquet - —Å–ø—Ä–∞–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∑–∞—è–≤–∫–∞–º
    - train_target_data.parquet - —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏
    - audiofiles/*.wav - –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏–º–µ–Ω–∏ YYYYMMDDHHMMSS_ID_–¢–µ–ª–µ—Ñ–æ–Ω,_–ö–æ–¥1,_–ö–æ–¥2.wav
    - svod.csv - —Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–≤—è–∑–µ–π
    """

    def __init__(self, data_dir: str = "data"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        Args:
            data_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        self.data_dir = Path(data_dir)

        # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.amplitude_dir = self.data_dir / "amplitude"
        self.audio_dir = self.data_dir / "audiofiles"
        self.svod_dir = self.data_dir

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        self.amplitude_dir.mkdir(parents=True, exist_ok=True)
        self.audio_dir.mkdir(parents=True, exist_ok=True)

    def load_amplitude_chunks(self) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ amplitude –¥–∞–Ω–Ω—ã—Ö

        Returns:
            DataFrame —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∑–≤–æ–Ω–∫–æ–≤
        """
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ amplitude —á–∞–Ω–∫–æ–≤...")

        # –ò—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º train_amplitude_chunk_XX.parquet
        chunk_files = list(self.amplitude_dir.glob("train_amplitude_chunk_*.parquet"))

        if not chunk_files:
            logger.warning("‚ö†Ô∏è  Amplitude —á–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return pd.DataFrame()

        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(chunk_files)} amplitude —á–∞–Ω–∫–æ–≤")

        all_chunks = []

        for chunk_file in sorted(chunk_files):
            try:
                logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º: {chunk_file.name}")
                chunk_df = pd.read_parquet(chunk_file)

                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞–Ω–∫–µ
                chunk_df['source_chunk'] = chunk_file.name

                all_chunks.append(chunk_df)
                logger.info(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(chunk_df)}")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {chunk_file}: {e}")
                continue

        if not all_chunks:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞")
            return pd.DataFrame()

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
        combined_df = pd.concat(all_chunks, ignore_index=True)
        logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(combined_df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        logger.info(f"üìã –ö–æ–ª–æ–Ω–∫–∏ amplitude –¥–∞–Ω–Ω—ã—Ö: {list(combined_df.columns)}")

        return combined_df

    def load_app_data(self) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞—è–≤–∫–∞–º

        Returns:
            DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞—è–≤–æ–∫
        """
        logger.info("üì± –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞—è–≤–æ–∫...")

        app_data_file = self.amplitude_dir / "train_app_data.parquet"

        if not app_data_file.exists():
            logger.warning(f"‚ö†Ô∏è  –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –∑–∞—è–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω: {app_data_file}")
            return pd.DataFrame()

        try:
            app_df = pd.read_parquet(app_data_file)
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(app_df)} –∑–∞–ø–∏—Å–µ–π –∑–∞—è–≤–æ–∫")
            logger.info(f"üìã –ö–æ–ª–æ–Ω–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞—è–≤–æ–∫: {list(app_df.columns)}")

            return app_df

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞—è–≤–æ–∫: {e}")
            return pd.DataFrame()

    def load_target_data(self) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫ (fraud/not fraud)

        Returns:
            DataFrame —Å —Ü–µ–ª–µ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        """
        logger.info("üéØ –ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫...")

        target_file = self.amplitude_dir / "train_target_data.parquet"

        if not target_file.exists():
            logger.warning(f"‚ö†Ô∏è  –§–∞–π–ª —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω: {target_file}")
            return pd.DataFrame()

        try:
            target_df = pd.read_parquet(target_file)
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(target_df)} —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫")
            logger.info(f"üìã –ö–æ–ª–æ–Ω–∫–∏ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {list(target_df.columns)}")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫
            if 'is_fraud' in target_df.columns:
                fraud_counts = target_df['is_fraud'].value_counts()
                fraud_rate = target_df['is_fraud'].mean()
                logger.info(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫: {fraud_counts.to_dict()}")
                logger.info(f"üìä –î–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {fraud_rate:.2%}")
            elif 'target' in target_df.columns:
                fraud_counts = target_df['target'].value_counts()
                fraud_rate = target_df['target'].mean()
                logger.info(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫: {fraud_counts.to_dict()}")
                logger.info(f"üìä –î–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {fraud_rate:.2%}")

            return target_df

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫: {e}")
            return pd.DataFrame()

    def load_svod_data(self) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–≤–æ–Ω–∫–æ–≤)

        Returns:
            DataFrame —Å–æ —Å–≤–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        logger.info("üìã –ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

        # –ò—â–µ–º —Å–≤–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã
        svod_files = []
        for pattern in ["svod.csv", "—Å–≤–æ–¥.csv", "*—Å–≤–æ–¥*.csv"]:
            svod_files.extend(list(self.svod_dir.glob(pattern)))

        if not svod_files:
            logger.warning("‚ö†Ô∏è  –°–≤–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return pd.DataFrame()

        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–≤–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(svod_files)}")

        all_svod = []

        for svod_file in svod_files:
            try:
                logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º: {svod_file.name}")

                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                for encoding in ['utf-8', 'cp1251', 'windows-1251']:
                    try:
                        svod_df = pd.read_csv(svod_file, encoding=encoding)
                        logger.info(f"  –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}")
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è {svod_file}")
                    continue

                svod_df['source_file'] = svod_file.name
                all_svod.append(svod_df)
                logger.info(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(svod_df)}")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {svod_file}: {e}")
                continue

        if not all_svod:
            return pd.DataFrame()

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        combined_svod = pd.concat(all_svod, ignore_index=True)
        logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(combined_svod)} —Å–≤–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"üìã –ö–æ–ª–æ–Ω–∫–∏ —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {list(combined_svod.columns)}")

        return combined_svod

    def parse_audio_filename(self, filename: str) -> Dict:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –∏–º–µ–Ω–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö

        –§–æ—Ä–º–∞—Ç: YYYYMMDDHHMMSS_ID_–¢–µ–ª–µ—Ñ–æ–Ω,_–ö–æ–¥1,_–ö–æ–¥2.wav
        –ü—Ä–∏–º–µ—Ä: 20241130151507_503121_77070094034,_500209,_500214.wav

        Args:
            filename: –ò–º—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            name_without_ext = filename.replace('.wav', '').replace('.mp3', '')

            # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è–º
            parts = name_without_ext.split('_')

            if len(parts) < 3:
                # –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–º—É
                return {
                    'call_id': name_without_ext,
                    'datetime': None,
                    'phone': None,
                    'codes': [],
                    'original_filename': filename,
                    'applicationid': None  # –ë—É–¥–µ—Ç —Å–≤—è–∑–∞–Ω —á–µ—Ä–µ–∑ —Å–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                }

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
            datetime_str = parts[0]
            try:
                call_datetime = datetime.strptime(datetime_str, '%Y%m%d%H%M%S')
            except:
                call_datetime = None

            # ID –∑–≤–æ–Ω–∫–∞ (–ù–ï applicationid!)
            call_id = parts[1]

            # –¢–µ–ª–µ—Ñ–æ–Ω –∏ –∫–æ–¥—ã (–º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∑–∞–ø—è—Ç—ã–µ)
            phone_and_codes = '_'.join(parts[2:])

            # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º
            phone_parts = phone_and_codes.split(',')
            phone = phone_parts[0] if phone_parts else None

            # –ö–æ–¥—ã (—É–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã)
            codes = [code.strip('_').strip() for code in phone_parts[1:] if code.strip('_').strip()]

            return {
                'call_id': call_id,
                'datetime': call_datetime,
                'phone': phone,
                'codes': codes,
                'original_filename': filename,
                'applicationid': None  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω –∏–∑ —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
            return {
                'call_id': filename.replace('.wav', '').replace('.mp3', ''),
                'datetime': None,
                'phone': None,
                'codes': [],
                'original_filename': filename,
                'applicationid': None
            }

    def get_audio_files_metadata(self) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤—Å–µ—Ö –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤

        Returns:
            DataFrame —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤
        """
        logger.info("üéµ –ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤...")

        if not self.audio_dir.exists():
            logger.warning(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.audio_dir}")
            return pd.DataFrame()

        # –ò—â–µ–º –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã
        audio_extensions = ['*.wav', '*.mp3', '*.flac']
        audio_files = []

        for ext in audio_extensions:
            audio_files.extend(list(self.audio_dir.glob(ext)))

        if not audio_files:
            logger.warning("‚ö†Ô∏è  –ê—É–¥–∏–æ—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return pd.DataFrame()

        logger.info(f"üéµ –ù–∞–π–¥–µ–Ω–æ {len(audio_files)} –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤")

        # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_list = []

        for audio_file in audio_files:
            metadata = self.parse_audio_filename(audio_file.name)
            metadata['file_path'] = str(audio_file)
            metadata['file_size'] = audio_file.stat().st_size
            metadata_list.append(metadata)

        metadata_df = pd.DataFrame(metadata_list)

        # –°–≤—è–∑—ã–≤–∞–µ–º –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã —Å APPLICATIONID —á–µ—Ä–µ–∑ —Å–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        metadata_df = self._link_audio_with_applicationid(metadata_df)

        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(metadata_df)}")
        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö call_id: {metadata_df['call_id'].nunique()}")

        return metadata_df

    def merge_all_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –µ–¥–∏–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç

        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–ø—Ä–∏–∑–Ω–∞–∫–∏, —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏)
        """
        logger.info("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        amplitude_data = self.load_amplitude_chunks()
        app_data = self.load_app_data()
        target_data = self.load_target_data()
        svod_data = self.load_svod_data()
        audio_metadata = self.get_audio_files_metadata()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        merge_keys = self._identify_merge_keys(amplitude_data, app_data, target_data, audio_metadata)

        logger.info(f"üîë –ö–ª—é—á–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {merge_keys}")

        # –ù–∞—á–∏–Ω–∞–µ–º —Å amplitude –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –æ—Å–Ω–æ–≤—ã
        merged_data = amplitude_data.copy()

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞—è–≤–æ–∫
        if not app_data.empty and merge_keys['app']:
            logger.info("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞—è–≤–æ–∫...")
            merged_data = pd.merge(
                merged_data, app_data,
                on=merge_keys['app'],
                how='left',
                suffixes=('', '_app')
            )
            logger.info(f"üìä –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å –∑–∞—è–≤–∫–∞–º–∏: {len(merged_data)} –∑–∞–ø–∏—Å–µ–π")

        # –î–æ–±–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if not audio_metadata.empty and merge_keys['audio']:
            logger.info("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∞—É–¥–∏–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏...")
            merged_data = pd.merge(
                merged_data, audio_metadata,
                on=merge_keys['audio'],
                how='left',
                suffixes=('', '_audio')
            )
            logger.info(f"üìä –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å –∞—É–¥–∏–æ: {len(merged_data)} –∑–∞–ø–∏—Å–µ–π")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if not svod_data.empty and merge_keys['svod']:
            logger.info("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–æ —Å–≤–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏...")
            merged_data = pd.merge(
                merged_data, svod_data,
                on=merge_keys['svod'],
                how='left',
                suffixes=('', '_svod')
            )
            logger.info(f"üìä –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–æ —Å–≤–æ–¥–æ–º: {len(merged_data)} –∑–∞–ø–∏—Å–µ–π")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ü–µ–ª–µ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        target_series = pd.Series(dtype=int)

        if not target_data.empty and merge_keys['target']:
            logger.info("üéØ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏...")

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ü–µ–ª–µ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ APPLICATIONID
            final_data = pd.merge(
                merged_data, target_data,
                on=merge_keys['target'],
                how='inner',  # –¢–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å –º–µ—Ç–∫–∞–º–∏
                suffixes=('', '_target')
            )

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            target_col = self._identify_target_column(target_data)
            if target_col:
                target_series = final_data[target_col]
                final_data = final_data.drop(columns=[target_col])

            merged_data = final_data
            logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(merged_data)} –∑–∞–ø–∏—Å–µ–π")
            logger.info(f"üéØ –¶–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫: {len(target_series)}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫
            if len(target_series) > 0:
                fraud_count = target_series.sum()
                fraud_rate = fraud_count / len(target_series)
                logger.info(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ {fraud_count}, –ª–µ–≥–∏—Ç–∏–º–Ω—ã–µ {len(target_series) - fraud_count}")
                logger.info(f"üìä –î–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {fraud_rate:.2%}")

        # –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        merged_data = self._clean_merged_data(merged_data)

        return merged_data, target_series

    def _identify_merge_keys(self, amplitude_data: pd.DataFrame, app_data: pd.DataFrame,
                           target_data: pd.DataFrame, audio_metadata: pd.DataFrame) -> Dict:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–π –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        """
        merge_keys = {}

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–≤—è–∑–∏
        primary_keys = ['APPLICATIONID', 'applicationid']
        secondary_keys = ['call_id', 'session_id', 'ID', 'id', 'user_id']

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞—Ö–æ–¥–∏–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∫–ª—é—á–∏
        for dataset_name, dataset in [
            ('app', app_data),
            ('target', target_data),
            ('audio', audio_metadata)
        ]:
            if dataset.empty:
                merge_keys[dataset_name] = None
                continue

            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º APPLICATIONID (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
            found_key = None

            for key in primary_keys:
                if key in amplitude_data.columns and key in dataset.columns:
                    found_key = key
                    break

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º —Å—Ä–µ–¥–∏ –≤—Ç–æ—Ä–∏—á–Ω—ã—Ö –∫–ª—é—á–µ–π
            if not found_key:
                for key in secondary_keys:
                    if key in amplitude_data.columns and key in dataset.columns:
                        found_key = key
                        break

            merge_keys[dataset_name] = found_key

        # –î–ª—è —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–≤—è–∑—å –∏–¥–µ—Ç —á–µ—Ä–µ–∑ –∏–º—è —Ñ–∞–π–ª–∞
        merge_keys['svod'] = None

        return merge_keys

    def _identify_target_column(self, target_data: pd.DataFrame) -> Optional[str]:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        """
        possible_target_cols = [
            'is_fraud', 'target', 'fraud', 'label', 'class',
            '–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '–º–µ—Ç–∫–∞'
        ]

        for col in possible_target_cols:
            if col in target_data.columns:
                return col

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∏—Å–ª–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É
        numeric_cols = target_data.select_dtypes(include=[np.number]).columns
        return numeric_cols[-1] if len(numeric_cols) > 0 else None

    def _clean_merged_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        –û—á–∏—Å—Ç–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        if data.empty:
            return data

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–æ–ª–æ–Ω–∫–∏
        data = data.loc[:, ~data.columns.duplicated()]

        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        cols_to_drop = [col for col in data.columns if
                       col.startswith('source_') or
                       col.endswith('_target') or
                       col in ['original_filename', 'file_path']]

        data = data.drop(columns=cols_to_drop, errors='ignore')

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        data[numeric_cols] = data[numeric_cols].fillna(0)

        categorical_cols = data.select_dtypes(include=['object']).columns
        data[categorical_cols] = data[categorical_cols].fillna('unknown')

        logger.info(f"üßπ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã: {data.shape}")

        return data

    def get_data_summary(self) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –≤—Å–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–º –¥–∞–Ω–Ω—ã–º

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞–Ω–Ω—ã—Ö
        """
        summary = {}

        # Amplitude —á–∞–Ω–∫–∏
        amplitude_files = list(self.amplitude_dir.glob("train_amplitude_chunk_*.parquet"))
        summary['amplitude_chunks'] = len(amplitude_files)

        # –î–∞–Ω–Ω—ã–µ –∑–∞—è–≤–æ–∫
        app_file = self.amplitude_dir / "train_app_data.parquet"
        summary['app_data_available'] = app_file.exists()

        # –¶–µ–ª–µ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        target_file = self.amplitude_dir / "train_target_data.parquet"
        summary['target_data_available'] = target_file.exists()

        # –ê—É–¥–∏–æ—Ñ–∞–π–ª—ã
        audio_files = []
        for ext in ['*.wav', '*.mp3', '*.flac']:
            audio_files.extend(list(self.audio_dir.glob(ext)))
        summary['audio_files_count'] = len(audio_files)

        # –°–≤–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã
        svod_files = []
        for pattern in ["svod.csv", "—Å–≤–æ–¥.csv", "*—Å–≤–æ–¥*.csv"]:
            svod_files.extend(list(self.svod_dir.glob(pattern)))
        summary['svod_files_count'] = len(svod_files)

        return summary

    def _link_audio_with_applicationid(self, audio_metadata: pd.DataFrame) -> pd.DataFrame:
        """
        –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ —Å APPLICATIONID —á–µ—Ä–µ–∑ —Å–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

        Args:
            audio_metadata: DataFrame —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ APPLICATIONID
        """
        logger.info("üîó –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ —Å APPLICATIONID...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        svod_data = self.load_svod_data()

        if svod_data.empty:
            logger.warning("‚ö†Ô∏è  –°–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–≤—è–∑–∫–∞ —á–µ—Ä–µ–∑ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
            return audio_metadata

        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–≤—è–∑–∫–∏ –≤ —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        filename_cols = []
        applicationid_cols = []

        for col in svod_data.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in ['—Ñ–∞–π–ª', 'file', 'name', '–∞—É–¥–∏–æ']):
                filename_cols.append(col)
            if any(keyword in col_lower for keyword in ['application', '–∑–∞—è–≤–∫–∞', '–Ω–æ–º–µ—Ä']):
                applicationid_cols.append(col)

        if not filename_cols or not applicationid_cols:
            logger.warning("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–≤—è–∑–∫–∏")
            logger.info(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(svod_data.columns)}")
            return audio_metadata

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        filename_col = filename_cols[0]
        applicationid_col = applicationid_cols[0]

        logger.info(f"üîë –°–≤—è–∑—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ –∫–æ–ª–æ–Ω–∫–∏: {filename_col} ‚Üí {applicationid_col}")

        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∏–º—è_—Ñ–∞–π–ª–∞ ‚Üí APPLICATIONID
        file_to_app_mapping = {}

        for _, row in svod_data.iterrows():
            filename = row[filename_col]
            app_id = row[applicationid_col]

            if pd.notna(filename) and pd.notna(app_id):
                # –û—á–∏—â–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –ø—É—Ç–∏ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                clean_filename = str(filename).split('/')[-1].split('\\')[-1]
                file_to_app_mapping[clean_filename] = str(app_id)

        logger.info(f"üìã –°–æ–∑–¥–∞–Ω –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è {len(file_to_app_mapping)} —Ñ–∞–π–ª–æ–≤")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫ –∞—É–¥–∏–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        audio_metadata['applicationid'] = audio_metadata['original_filename'].map(file_to_app_mapping)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–≤—è–∑—ã–≤–∞–Ω–∏—è
        linked_count = audio_metadata['applicationid'].notna().sum()
        total_count = len(audio_metadata)

        logger.info(f"‚úÖ –°–≤—è–∑–∞–Ω–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤: {linked_count}/{total_count} ({linked_count/total_count:.1%})")

        if linked_count == 0:
            logger.error("‚ùå –ù–∏ –æ–¥–∏–Ω –∞—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –±—ã–ª —Å–≤—è–∑–∞–Ω —Å APPLICATIONID!")
            logger.info("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ –≤ —Å–≤–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ audiofiles")

        return audio_metadata

    def create_test_prediction_data(self, output_dir: str):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–±–µ–∑ —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫)

        Args:
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info("üß™ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±–µ–∑ target_data)
        amplitude_data = self.load_amplitude_chunks()
        app_data = self.load_app_data()
        audio_metadata = self.get_audio_files_metadata()

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–µ–∑ —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫
        merged_data = amplitude_data.copy()

        if not app_data.empty:
            merge_key = self._find_common_column(amplitude_data, app_data)
            if merge_key:
                merged_data = pd.merge(merged_data, app_data, on=merge_key, how='left')

        if not audio_metadata.empty:
            merge_key = self._find_common_column(amplitude_data, audio_metadata)
            if merge_key:
                merged_data = pd.merge(merged_data, audio_metadata, on=merge_key, how='left')

        # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        merged_data = self._clean_merged_data(merged_data)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        prediction_file = output_path / "prediction_data.parquet"
        merged_data.to_parquet(prediction_file)

        logger.info(f"‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {prediction_file}")
        logger.info(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {merged_data.shape}")

    def _find_common_column(self, df1: pd.DataFrame, df2: pd.DataFrame) -> Optional[str]:
        """
        –ü–æ–∏—Å–∫ –æ–±—â–µ–π –∫–æ–ª–æ–Ω–∫–∏ –º–µ–∂–¥—É –¥–≤—É–º—è DataFrame
        """
        common_cols = set(df1.columns) & set(df2.columns)

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        priority_cols = ['session_id', 'ID', 'id', 'APPLICATIONID']

        for col in priority_cols:
            if col in common_cols:
                return col

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é –æ–±—â—É—é –∫–æ–ª–æ–Ω–∫—É
        return list(common_cols)[0] if common_cols else None
