# ü§ñ –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM

–ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM –¥–ª—è –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ —Å–∏—Å—Ç–µ–º—ã —á–µ—Ä–µ–∑ Ollama.

## üéØ –ó–∞—á–µ–º –ª–æ–∫–∞–ª—å–Ω–∞—è LLM?

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- ‚úÖ **–ë–µ—Å–ø–ª–∞—Ç–Ω–æ** - –Ω–∏–∫–∞–∫–∏—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ API
- ‚úÖ **–ü—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å** - –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç –≤–∞—à—É —Å–∏—Å—Ç–µ–º—É
- ‚úÖ **–ê–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å** - —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
- ‚úÖ **–ö–æ–Ω—Ç—Ä–æ–ª—å** - –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –º–æ–¥–µ–ª—å—é
- ‚úÖ **–°–∫–æ—Ä–æ—Å—Ç—å** - –Ω–µ—Ç –∑–∞–¥–µ—Ä–∂–µ–∫ —Å–µ—Ç–∏

### –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ OpenAI API:
- ‚ùå –°—Ç–æ–∏–º–æ—Å—Ç—å: $2-60 –∑–∞ 1000 –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
- ‚ùå –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
- ‚ùå –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ OpenAI
- ‚ùå –õ–∏–º–∏—Ç—ã –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã

## üöÄ –ë—ã—Å—Ç—Ä–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

#### macOS:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Windows:
1. –°–∫–∞—á–∞–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫: https://ollama.ai/download
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ .exe —Ñ–∞–π–ª
3. –°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º

#### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ (—á–µ—Ä–µ–∑ Docker):
```bash
docker pull ollama/ollama
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

### –®–∞–≥ 2: –ó–∞–ø—É—Å–∫ Ollama —Å–µ—Ä–≤–µ—Ä–∞

```bash
ollama serve
```

**–í–∞–∂–Ω–æ**: –û—Å—Ç–∞–≤—å—Ç–µ —ç—Ç–æ—Ç —Ç–µ—Ä–º–∏–Ω–∞–ª –æ—Ç–∫—Ä—ã—Ç—ã–º - —Å–µ—Ä–≤–µ—Ä –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ.

### –®–∞–≥ 3: –í—ã–±–æ—Ä –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

#### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏:

1. **Llama 3.2 3B** (–õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å):
```bash
ollama pull llama3.2:3b
```
- –†–∞–∑–º–µ—Ä: ~6GB
- –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: 8GB RAM
- –ö–∞—á–µ—Å—Ç–≤–æ: –û—Ç–ª–∏—á–Ω–æ–µ

2. **Phi-3.5 Mini** (–ë—ã—Å—Ç—Ä–∞—è):
```bash
ollama pull phi3.5
```
- –†–∞–∑–º–µ—Ä: ~4GB
- –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: 6GB RAM
- –ö–∞—á–µ—Å—Ç–≤–æ: –•–æ—Ä–æ—à–µ–µ

3. **Qwen2.5 7B** (–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ):
```bash
ollama pull qwen2.5:7b
```
- –†–∞–∑–º–µ—Ä: ~8GB
- –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: 12GB RAM
- –ö–∞—á–µ—Å—Ç–≤–æ: –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞

### –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
ollama list

# –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
ollama run llama3.2:3b "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
```

## üìã –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ:
- **RAM**: 6GB —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏
- **–î–∏—Å–∫**: 10GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞
- **CPU**: –õ—é–±–æ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ:
- **RAM**: 12GB+ (–¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
- **–î–∏—Å–∫**: SSD –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
- **GPU**: NVIDIA GPU —Å CUDA (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤:

#### –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM:
```bash
# Linux/macOS
free -h

# Windows
wmic memorychip get size
```

#### –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–∫–∞:
```bash
# Linux/macOS
df -h

# Windows
dir
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ —Å–∏—Å—Ç–µ–º—ã

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:

```bash
pip install requests psutil
```

### –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:

```bash
cd hackathon
python3 scripts/predict_local_llm.py
```

### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ —Ä–µ—Å—É—Ä—Å–∞–º:

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –≤ –∫–æ–¥–µ
import psutil

ram_gb = psutil.virtual_memory().total / (1024**3)

if ram_gb >= 16:
    model = "qwen2.5:7b"      # –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
elif ram_gb >= 8:
    model = "llama3.2:3b"     # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π
else:
    model = "phi3.5"          # –≠–∫–æ–Ω–æ–º–∏—á–Ω—ã–π
```

## üé® –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:

```python
from src.local_llm_enhancer import LocalLLMEnhancer

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
llm = LocalLLMEnhancer(model="llama3.2:3b")

# –û–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è
explanation = llm.explain_fraud_decision(
    user_data={'user_id': 'user_123'},
    features={'night_activity_ratio': 0.8},
    probability=0.85
)

print(explanation.explanation)
# "–í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –∏–∑-–∑–∞ –Ω–µ–æ–±—ã—á–Ω–æ–π 
#  –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –Ω–æ—á–Ω–æ–µ –≤—Ä–µ–º—è (80%)"
```

### –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω:

```bash
# 1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
python3 scripts/main.py --train

# 2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM
python3 scripts/predict_local_llm.py
```

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | RAM | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | –†—É—Å—Å–∫–∏–π |
|--------|--------|-----|----------|----------|---------|
| phi3.5 | 4GB | 6GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| llama3.2:3b | 6GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| qwen2.5:7b | 8GB | 12GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É:

- **–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞/MVP**: phi3.5 (–±—ã—Å—Ç—Ä–æ, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
- **–ü—Ä–æ–¥–∞–∫—à–µ–Ω**: llama3.2:3b (–±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏)  
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ**: qwen2.5:7b (–µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç —Ä–µ—Å—É—Ä—Å–æ–≤)

## ‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama:

```bash
# –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
export OLLAMA_NUM_PARALLEL=4

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU (–µ—Å–ª–∏ –µ—Å—Ç—å NVIDIA)
export OLLAMA_GPU_LAYERS=35

# –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
export OLLAMA_MAX_CONTEXT=4096
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –∫–æ–¥–µ:

```python
# –í local_llm_enhancer.py
payload = {
    "model": self.model,
    "prompt": prompt,
    "options": {
        "num_predict": 200,     # –ö–æ—Ä–æ—á–µ –æ—Ç–≤–µ—Ç—ã = –±—ã—Å—Ç—Ä–µ–µ
        "temperature": 0.3,     # –ú–µ–Ω–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ = —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
        "top_p": 0.9,          # –§–æ–∫—É—Å –Ω–∞ –ª—É—á—à–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö
        "num_ctx": 2048        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
    }
}
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤:

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM
htop

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU (–µ—Å–ª–∏ –µ—Å—Ç—å)
nvidia-smi

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Ollama
curl http://localhost:11434/api/ps
```

## üêõ –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:

#### 1. "Ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É
which ollama

# –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
curl -fsSL https://ollama.ai/install.sh | sh
```

#### 2. "–°–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
curl http://localhost:11434/api/tags

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä
pkill ollama
ollama serve
```

#### 3. "–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
```bash
# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
ollama list

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å
ollama pull llama3.2:3b
```

#### 4. "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏"
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM
free -h

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å
ollama pull phi3.5
```

#### 5. "–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞"
```bash
# –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤
export OLLAMA_MAX_TOKENS=150

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU
export OLLAMA_GPU_LAYERS=20

# –ú–µ–Ω—å—à–∞—è –º–æ–¥–µ–ª—å
ollama pull phi3.5
```

### –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:

```python
# –¢–µ—Å—Ç –≤ –∫–æ–¥–µ
from src.local_llm_enhancer import LocalLLMEnhancer, OllamaInstaller

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
print("Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞:", OllamaInstaller.check_ollama_installed())

# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –º–æ–¥–µ–ª—å
print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –º–æ–¥–µ–ª—å:", OllamaInstaller.recommend_model_by_resources())

# –¢–µ—Å—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
llm = LocalLLMEnhancer("llama3.2:3b")
print("–¢–µ—Å—Ç –ø—Ä–æ—à–µ–ª:", llm.test_connection())
```

## üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Ollama:

```bash
# –û–±–Ω–æ–≤–∏—Ç—å Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# –û–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å
ollama pull llama3.2:3b
```

### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏:

```bash
# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
ollama list

# –£–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å
ollama rm old-model

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
ollama show llama3.2:3b
```

### –û—á–∏—Å—Ç–∫–∞ –º–µ—Å—Ç–∞:

```bash
# –ù–∞–π—Ç–∏ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã Ollama
du -h ~/.ollama

# –£–¥–∞–ª–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏
ollama rm unused-model
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏

### –õ–æ–≥–∏ Ollama:

```bash
# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ª–æ–≥–∏
tail -f ~/.ollama/logs/server.log

# –°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–µ—Ä–∞
curl http://localhost:11434/api/tags
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –∫–æ–¥–µ:

```python
# –í –≤–∞—à–µ–º –∫–æ–¥–µ –¥–æ–±–∞–≤—å—Ç–µ
import time
import logging

start_time = time.time()
explanation = llm.explain_fraud_decision(...)
end_time = time.time()

logging.info(f"LLM –æ—Ç–≤–µ—Ç –∑–∞ {end_time - start_time:.2f} —Å–µ–∫")
```

## üîß –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã

### 1. LM Studio (GUI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)
- –°–∫–∞—á–∞—Ç—å: https://lmstudio.ai/
- –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- –õ–µ–≥–∫–æ —É–ø—Ä–∞–≤–ª—è—Ç—å –º–æ–¥–µ–ª—è–º–∏

### 2. Text Generation WebUI
```bash
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
pip install -r requirements.txt
```

### 3. GPT4All (–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π)
```bash
pip install gpt4all
```

```python
from gpt4all import GPT4All
model = GPT4All("orca-mini-3b-gguf2-q4_0.gguf")
response = model.generate("Explain fraud detection")
```

## üìã –ß–µ–∫-–ª–∏—Å—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏

- [ ] Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ (`ollama --version`)
- [ ] –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω (`ollama serve`)
- [ ] –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (`ollama list`)
- [ ] –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ (`ollama run model "test"`)
- [ ] Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (`pip install requests`)
- [ ] –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ (`python3 scripts/predict_local_llm.py`)
- [ ] –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM (>6GB —Å–≤–æ–±–æ–¥–Ω–æ–π)
- [ ] –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∏—Å–∫–∞ (>10GB)

## üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã

–ü–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ:

### –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:
- `fraud_predictions_local_llm.csv` - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
- `fraud_report_local_llm.txt` - —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
- `fraud_report_local_llm.html` - –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π HTML –æ—Ç—á–µ—Ç

### –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è:
```
User user_123 - –†–∏—Å–∫: 85%
–û–±—ä—è—Å–Ω–µ–Ω–∏–µ: –í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –∏–∑-–∑–∞ –Ω–µ–æ–±—ã—á–Ω–æ–π 
–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –Ω–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –∏ –∫–æ—Ä–æ—Ç–∫–æ–π –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ—Å—Å–∏–π.

–ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã:
‚Ä¢ –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–æ—á—å—é: 0.80
‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Å—Å–∏–π: 3.00
‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏: 45.00

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
‚Ä¢ –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
‚Ä¢ –°–≤—è–∑–∞—Ç—å—Å—è —Å –∫–ª–∏–µ–Ω—Ç–æ–º –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
```

## üÜò –ü–æ–¥–¥–µ—Ä–∂–∫–∞

### –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/README.md)
- [Model Library](https://ollama.ai/library)

### –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏:
- [Discord —Å–æ–æ–±—â–µ—Å—Ç–≤–æ Ollama](https://discord.gg/ollama)
- [GitHub Issues](https://github.com/ollama/ollama/issues)

### –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:
1. –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ –∫–æ–º–ø—å—é—Ç–µ—Ä
2. –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama
3. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –º–æ–¥–µ–ª—å phi3.5 (—Å–∞–º–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–∞—è)
4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–Ω—Ç–∏–≤–∏—Ä—É—Å (–º–æ–∂–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å)

---

**–ì–æ—Ç–æ–≤–æ!** üéâ 

–í–∞—à–∞ –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ —Å–∏—Å—Ç–µ–º–∞ —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM:

```bash
# –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM
python3 scripts/predict_local_llm.py
```

üîí –í—Å–µ –¥–∞–Ω–Ω—ã–µ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–∞ –≤–∞—à–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ!
üí∞ –ù–∏–∫–∞–∫–∏—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ API!
‚ö° –†–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞!