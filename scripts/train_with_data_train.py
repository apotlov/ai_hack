#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ data_train
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent.parent / "src"))

import logging
import pandas as pd
import numpy as np
from tqdm import tqdm
from typing import Dict, Any

from real_features_processor import RealFeaturesProcessor
from model_trainer import ModelTrainer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def validate_data_structure(data_dir: Path) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

    Args:
        data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏

    Returns:
        True –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞
    """
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö...")

    amplitude_dir = data_dir / "amplitude"
    audio_dir = data_dir / "audiofiles"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    required_files = [
        "train_target_data.parquet",  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    ]

    missing_files = []
    for file_name in required_files:
        file_path = amplitude_dir / file_name
        if not file_path.exists():
            missing_files.append(file_name)

    if missing_files:
        logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã: {missing_files}")
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º amplitude —á–∞–Ω–∫–∏
    amplitude_chunks = list(amplitude_dir.glob("train_amplitude_chunk_*.parquet"))
    if not amplitude_chunks:
        logger.warning("‚ö†Ô∏è  Amplitude —á–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –Ω–æ —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ")
    else:
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ amplitude —á–∞–Ω–∫–æ–≤: {len(amplitude_chunks)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞—è–≤–æ–∫
    app_data_file = amplitude_dir / "train_app_data.parquet"
    if app_data_file.exists():
        logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞—è–≤–æ–∫ –Ω–∞–π–¥–µ–Ω—ã")
    else:
        logger.warning("‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –∑–∞—è–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã
    if audio_dir.exists():
        audio_files = list(audio_dir.glob("*.wav"))
        logger.info(f"üéµ –ù–∞–π–¥–µ–Ω–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤: {len(audio_files)}")
    else:
        logger.warning("‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º svod.csv –≤ –∫–æ—Ä–Ω–µ data_train
    svod_file = data_dir / "svod.csv"
    if svod_file.exists():
        logger.info("‚úÖ –§–∞–π–ª svod.csv –Ω–∞–π–¥–µ–Ω –≤ –∫–æ—Ä–Ω–µ")
    else:
        logger.warning("‚ö†Ô∏è  –§–∞–π–ª svod.csv –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ—Ä–Ω–µ")

    return True


def analyze_data_quality(X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö

    Args:
        X: –ü—Ä–∏–∑–Ω–∞–∫–∏
        y: –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    logger.info("üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")

    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    n_samples, n_features = X.shape
    fraud_rate = y.mean()
    missing_ratio = X.isnull().sum().sum() / (n_samples * n_features)

    logger.info(f"üìà –û–±—Ä–∞–∑—Ü–æ–≤: {n_samples}")
    logger.info(f"üìä –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {n_features}")
    logger.info(f"üéØ –î–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {fraud_rate:.2%}")
    logger.info(f"üîç –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {missing_ratio:.2%}")

    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    if fraud_rate < 0.01:
        logger.warning(f"‚ö†Ô∏è  –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –¥–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {fraud_rate:.2%}")
    elif fraud_rate > 0.5:
        logger.warning(f"‚ö†Ô∏è  –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è –¥–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {fraud_rate:.2%}")

    if missing_ratio > 0.3:
        logger.warning(f"‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {missing_ratio:.2%}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    class_balance = y.value_counts().min() / y.value_counts().max()
    if class_balance < 0.1:
        logger.warning(f"‚ö†Ô∏è  –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {fraud_rate:.2%}")

    return {
        'n_samples': n_samples,
        'n_features': n_features,
        'fraud_rate': fraud_rate,
        'missing_values_ratio': missing_ratio,
        'class_balance': class_balance
    }


def prepare_training_data(features_processor: RealFeaturesProcessor) -> tuple:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

    Args:
        features_processor: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (X, y, metadata)
    """
    logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")

    with tqdm(total=4, desc="üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö", unit="—à–∞–≥", leave=False) as pbar:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        pbar.set_description("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        X, y = features_processor.combine_all_features()
        pbar.update(1)

        if X.empty or y.empty:
            raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

        # –£–¥–∞–ª—è–µ–º session_id –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        pbar.set_description("üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        if 'session_id' in X.columns:
            X = X.drop('session_id', axis=1)
        pbar.update(1)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        pbar.set_description("üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞")
        quality_metrics = analyze_data_quality(X, y)
        pbar.update(1)

        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        pbar.set_description("‚ö†Ô∏è  –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞")
        if quality_metrics['missing_values_ratio'] > 0.2:
            logger.warning("‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

        if quality_metrics['fraud_rate'] < 0.05:
            logger.warning("‚ö†Ô∏è  –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –¥–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞")
        elif quality_metrics['fraud_rate'] > 0.5:
            logger.warning("‚ö†Ô∏è  –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è –¥–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞")
        pbar.update(1)

    return X, y, quality_metrics


def train_antifraud_model(X: pd.DataFrame, y: pd.Series, models_dir: Path, quality_metrics: Dict) -> Dict:
    """
    –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏

    Args:
        X: –ü—Ä–∏–∑–Ω–∞–∫–∏
        y: –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏
        models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        quality_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö

    Returns:
        –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    """
    logger.info("ü§ñ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞
    model_trainer = ModelTrainer(str(models_dir))

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if quality_metrics['fraud_rate'] < 0.1:
        logger.info("üéØ –ù–∞—Å—Ç—Ä–æ–µ–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    metrics = model_trainer.train(X, y, test_size=0.2)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_name = "real_antifraud_model"
    model_trainer.save_model(model_name)

    logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_name}")

    return metrics


def validate_antifraud_model(model_trainer: ModelTrainer, X: pd.DataFrame, y: pd.Series) -> Dict:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

    Args:
        model_trainer: –û–±—É—á–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –º–æ–¥–µ–ª–∏
        X: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        y: –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

    Returns:
        –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏...")

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = model_trainer.predict(X)
    probabilities = model_trainer.predict_proba(X)

    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

    try:
        auc_score = roc_auc_score(y, probabilities[:, 1])
        logger.info(f"üìä AUC-ROC: {auc_score:.3f}")
    except:
        auc_score = 0.0
        logger.warning("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å AUC-ROC")

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y, predictions)
    logger.info(f"üìä –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:\n{cm}")

    # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    report = classification_report(y, predictions, output_dict=True)
    logger.info("üìä –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
    logger.info(classification_report(y, predictions))

    return {
        'auc_roc': auc_score,
        'classification_report': report,
        'confusion_matrix': cm.tolist()
    }


def create_training_report(quality_metrics: Dict, training_metrics: Dict,
                         validation_metrics: Dict, output_dir: Path) -> None:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏

    Args:
        quality_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        training_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        validation_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞
    """
    logger.info("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏...")

    report_content = f"""
=== –û–¢–ß–ï–¢ –û–ë –û–ë–£–ß–ï–ù–ò–ò –ê–ù–¢–ò–§–†–û–î –ú–û–î–ï–õ–ò ===
–î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: data_train/

–ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:
- –û–±—Ä–∞–∑—Ü–æ–≤: {quality_metrics['n_samples']:,}
- –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {quality_metrics['n_features']:,}
- –î–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {quality_metrics['fraud_rate']:.2%}
- –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {quality_metrics['missing_values_ratio']:.2%}
- –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {quality_metrics['class_balance']:.3f}

–ú–ï–¢–†–ò–ö–ò –û–ë–£–ß–ï–ù–ò–Ø:
- AUC-ROC (–æ–±—É—á–µ–Ω–∏–µ): {training_metrics.get('auc_roc', 'N/A')}
- Precision (–æ–±—É—á–µ–Ω–∏–µ): {training_metrics.get('precision', 'N/A')}
- Recall (–æ–±—É—á–µ–Ω–∏–µ): {training_metrics.get('recall', 'N/A')}
- F1-Score (–æ–±—É—á–µ–Ω–∏–µ): {training_metrics.get('f1_score', 'N/A')}

–ú–ï–¢–†–ò–ö–ò –í–ê–õ–ò–î–ê–¶–ò–ò:
- AUC-ROC (–≤–∞–ª–∏–¥–∞—Ü–∏—è): {validation_metrics.get('auc_roc', 'N/A')}

–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
"""

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫
    if quality_metrics['fraud_rate'] < 0.02:
        report_content += "\n- ‚ö†Ô∏è  –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –¥–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏"

    if quality_metrics['missing_values_ratio'] > 0.1:
        report_content += "\n- ‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π - —É–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö"

    if training_metrics.get('auc_roc', 0) > 0.8:
        report_content += "\n- ‚úÖ –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏"
    elif training_metrics.get('auc_roc', 0) > 0.7:
        report_content += "\n- üìä –ü—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏"
    else:
        report_content += "\n- ‚ö†Ô∏è  –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ - —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ"

    report_content += f"""

–°–¢–ê–¢–£–° –ú–û–î–ï–õ–ò: ‚úÖ –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ
–§–∞–π–ª –º–æ–¥–µ–ª–∏: models/real_antifraud_model.joblib

=== –ö–û–ù–ï–¶ –û–¢–ß–ï–¢–ê ===
"""

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    report_file = output_dir / "training_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)

    logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ data_train
    """
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ data_train")

    try:
        # –û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        with tqdm(total=7, desc="üöÄ –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏", unit="—ç—Ç–∞–ø", position=0) as main_pbar:

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π - –ò–°–ü–û–õ–¨–ó–£–ï–ú data_train
            main_pbar.set_description("üìÇ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π")
            data_dir = Path(__file__).parent.parent / "data_train"  # –ò–ó–ú–ï–ù–ï–ù–û: data_train –≤–º–µ—Å—Ç–æ data
            models_dir = Path(__file__).parent.parent / "models"
            output_dir = Path(__file__).parent.parent / "output"

            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            models_dir.mkdir(exist_ok=True)
            output_dir.mkdir(exist_ok=True)

            logger.info(f"üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö: {data_dir}")
            logger.info(f"ü§ñ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {models_dir}")
            logger.info(f"üìä –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            main_pbar.update(1)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
            main_pbar.set_description("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
            if not validate_data_structure(data_dir):
                logger.error("‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞")
                return False
            main_pbar.update(1)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        features_processor = RealFeaturesProcessor(str(data_dir))

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        main_pbar.set_description("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        X, y, quality_metrics = prepare_training_data(features_processor)
        main_pbar.update(1)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        main_pbar.set_description("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        if len(X) < 100:
            logger.error("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 100 –æ–±—Ä–∞–∑—Ü–æ–≤)")
            return False

        # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        problematic_cols = []
        for col in X.columns:
            col_dtype = str(X[col].dtype)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ç–∏–ø—ã
            if any(dtype_name in col_dtype.lower() for dtype_name in ['object', 'datetime', 'timedelta', 'string']):
                problematic_cols.append(col)
                logger.warning(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col} (—Ç–∏–ø: {col_dtype})")

        if problematic_cols:
            logger.error(f"‚ùå –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {problematic_cols}")
            logger.info("üîß –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è...")

            for col in problematic_cols:
                col_dtype = str(X[col].dtype)
                try:
                    if 'datetime' in col_dtype.lower():
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º datetime –≤ timestamp
                        X[col] = pd.to_datetime(X[col]).astype('int64') // 10**9
                        logger.info(f"‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ datetime –∫–æ–ª–æ–Ω–∫–∞: {col}")
                    elif 'timedelta' in col_dtype.lower():
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timedelta –≤ —Å–µ–∫—É–Ω–¥—ã
                        X[col] = pd.to_timedelta(X[col]).dt.total_seconds()
                        logger.info(f"‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ timedelta –∫–æ–ª–æ–Ω–∫–∞: {col}")
                    else:
                        # –ü—Ä–æ–±—É–µ–º —á–∏—Å–ª–æ–≤–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
                        X[col] = pd.to_numeric(X[col], errors='coerce')
                        X[col] = X[col].fillna(0)
                        logger.info(f"‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {col}")
                except Exception as e:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å {col}: {e}")
                    X = X.drop(columns=[col])
                    logger.info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col}")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã
        final_numeric_types = ['int', 'float', 'number']
        valid_cols = []

        for col in X.columns:
            col_dtype = str(X[col].dtype).lower()
            if any(num_type in col_dtype for num_type in final_numeric_types):
                valid_cols.append(col)
            else:
                logger.warning(f"üóëÔ∏è  –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–º —Ç–∏–ø–æ–º: {col} ({X[col].dtype})")

        if len(valid_cols) != len(X.columns):
            X = X[valid_cols]
            logger.info(f"üîß –û—Å—Ç–∞–≤–ª–µ–Ω—ã —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {X.shape}")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∏ Inf
        X = X.replace([np.inf, -np.inf], 0)
        X = X.fillna(0)

        logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {X.shape}, –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —á–∏—Å–ª–æ–≤—ã–µ")
        logger.info(f"üìä –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö: {X.dtypes.value_counts().to_dict()}")
        main_pbar.update(1)

        if y.nunique() < 2:
            logger.error("‚ùå –ù–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–∫–∞—Ö")
            return False

        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        main_pbar.set_description("ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
        training_metrics = train_antifraud_model(X, y, models_dir, quality_metrics)
        main_pbar.update(1)

        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        main_pbar.set_description("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
        model_trainer = ModelTrainer(str(models_dir))
        model_trainer.load_model("real_antifraud_model")
        validation_metrics = validate_antifraud_model(model_trainer, X, y)
        main_pbar.update(1)

        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        main_pbar.set_description("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞")
        create_training_report(quality_metrics, training_metrics, validation_metrics, output_dir)
        main_pbar.update(1)

        logger.info("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/real_antifraud_model.joblib")
        logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: output/training_report.txt")

        return True

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        logger.error("Traceback (most recent call last):")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
