#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ —Å–∏—Å—Ç–µ–º—ã
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent.parent / "src"))

import logging
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional

from real_features_processor import RealFeaturesProcessor
from model_trainer import ModelTrainer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def validate_prediction_data(data_dir: Path) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

    Args:
        data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

    Returns:
        True –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")

    amplitude_dir = data_dir / "amplitude"

    # –ò—â–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–º–æ–≥—É—Ç –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è test_* –∏–ª–∏ predict_*)
    data_patterns = [
        "test_amplitude_chunk_*.parquet",
        "predict_amplitude_chunk_*.parquet",
        "amplitude_chunk_*.parquet"
    ]

    found_files = []
    for pattern in data_patterns:
        found_files.extend(list(amplitude_dir.glob(pattern)))

    if found_files:
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö: {len(found_files)}")
        return True

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Ñ–∞–π–ª—ã (–¥–ª—è –¥–µ–º–æ)
    train_files = list(amplitude_dir.glob("train_amplitude_chunk_*.parquet"))
    if train_files:
        logger.warning("‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω—ã —Ç–æ–ª—å–∫–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Ñ–∞–π–ª—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")
        return True

    logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
    return False


def load_trained_model(models_dir: Path) -> ModelTrainer:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

    Args:
        models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—è–º–∏

    Returns:
        –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    logger.info("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")

    model_trainer = ModelTrainer(str(models_dir))

    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    model_names = [
        "real_antifraud_model",
        "antifraud_model_v1",
        "antifraud_model"
    ]

    for model_name in model_names:
        try:
            model_trainer.load_model(model_name)
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
            return model_trainer
        except FileNotFoundError:
            continue

    raise FileNotFoundError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ.")


def prepare_prediction_features(features_processor: RealFeaturesProcessor,
                               prediction_data_dir: str) -> pd.DataFrame:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

    Args:
        features_processor: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        prediction_data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

    Returns:
        DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    features_df = features_processor.create_prediction_features(prediction_data_dir)

    if features_df.empty:
        raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")

    # –£–¥–∞–ª—è–µ–º session_id –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ)
    session_ids = None
    if 'session_id' in features_df.columns:
        session_ids = features_df['session_id'].copy()
        features_df = features_df.drop('session_id', axis=1)

    logger.info(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_df.shape}")

    return features_df, session_ids


def make_predictions(model_trainer: ModelTrainer, X: pd.DataFrame) -> Dict[str, Any]:
    """
    –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

    Args:
        model_trainer: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        X: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    """
    logger.info("üîÆ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    predictions = model_trainer.predict(X)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    probabilities = model_trainer.predict_proba(X)
    fraud_probabilities = probabilities[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = {
        'predictions': predictions,
        'fraud_probabilities': fraud_probabilities,
        'total_samples': len(X),
        'predicted_fraud_count': int(predictions.sum()),
        'predicted_fraud_rate': float(predictions.mean()),
        'avg_fraud_probability': float(fraud_probabilities.mean()),
        'max_fraud_probability': float(fraud_probabilities.max()),
        'min_fraud_probability': float(fraud_probabilities.min()),
        'high_risk_count': int((fraud_probabilities >= 0.7).sum()),
        'medium_risk_count': int(((fraud_probabilities >= 0.3) & (fraud_probabilities < 0.7)).sum()),
        'low_risk_count': int((fraud_probabilities < 0.3).sum())
    }

    return results


def create_prediction_report(results: Dict[str, Any], X: pd.DataFrame,
                           session_ids: Optional[pd.Series], output_dir: Path) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        X: –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        session_ids: ID —Å–µ—Å—Å–∏–π
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

    Returns:
        DataFrame —Å –æ—Ç—á–µ—Ç–æ–º
    """
    logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏...")

    # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    report_df = pd.DataFrame({
        'sample_id': range(len(X)),
        'prediction': results['predictions'],
        'fraud_probability': results['fraud_probabilities'],
        'risk_level': pd.cut(
            results['fraud_probabilities'],
            bins=[0, 0.3, 0.7, 1.0],
            labels=['–ù–∏–∑–∫–∏–π', '–°—Ä–µ–¥–Ω–∏–π', '–í—ã—Å–æ–∫–∏–π']
        )
    })

    # –î–æ–±–∞–≤–ª—è–µ–º session_ids –µ—Å–ª–∏ –µ—Å—Ç—å
    if session_ids is not None:
        report_df.insert(0, 'session_id', session_ids.values)

    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    report_df['prediction_datetime'] = pd.Timestamp.now()

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    feature_cols_to_include = []

    # –ò—â–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    important_patterns = ['amplitude_', 'audio_', 'app_', 'temporal_']
    for pattern in important_patterns:
        matching_cols = [col for col in X.columns if col.startswith(pattern)]
        if matching_cols:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
            feature_cols_to_include.extend(matching_cols[:3])

    # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –æ—Ç—á–µ—Ç
    for col in feature_cols_to_include[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 –∫–æ–ª–æ–Ω–∫–∞–º–∏
        if col in X.columns:
            report_df[f'feature_{col}'] = X[col].values

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ (–æ—Ç –≤—ã—Å–æ–∫–æ–π –∫ –Ω–∏–∑–∫–æ–π)
    report_df = report_df.sort_values('fraud_probability', ascending=False)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    detailed_report_path = output_dir / "fraud_predictions_detailed.csv"
    report_df.to_csv(detailed_report_path, index=False, encoding='utf-8')
    logger.info(f"üíæ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {detailed_report_path}")

    return report_df


def analyze_predictions(results: Dict[str, Any], report_df: pd.DataFrame) -> str:
    """
    –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        report_df: –û—Ç—á–µ—Ç —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏

    Returns:
        –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    """
    analysis = f"""
=== –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô ===
–î–∞—Ç–∞: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
- –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {results['total_samples']:,}
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ —Å–ª—É—á–∞–µ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {results['predicted_fraud_count']:,}
- –û–±—â–∞—è –¥–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {results['predicted_fraud_rate']:.2%}

–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –£–†–û–í–ù–Ø–ú –†–ò–°–ö–ê:
- üî¥ –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ (‚â•70%): {results['high_risk_count']:,} ({results['high_risk_count']/results['total_samples']:.1%})
- üü° –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ (30-70%): {results['medium_risk_count']:,} ({results['medium_risk_count']/results['total_samples']:.1%})
- üü¢ –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫ (<30%): {results['low_risk_count']:,} ({results['low_risk_count']/results['total_samples']:.1%})

–í–ï–†–û–Ø–¢–ù–û–°–¢–ò –ú–û–®–ï–ù–ù–ò–ß–ï–°–¢–í–ê:
- –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {results['avg_fraud_probability']:.3f} ({results['avg_fraud_probability']:.1%})
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {results['max_fraud_probability']:.3f} ({results['max_fraud_probability']:.1%})
- –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {results['min_fraud_probability']:.3f} ({results['min_fraud_probability']:.1%})

–¢–û–ü-10 –°–ê–ú–´–• –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–• –°–õ–£–ß–ê–ï–í:
"""

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø —Å–ª—É—á–∞–∏
    top_cases = report_df.head(10)
    for i, (_, case) in enumerate(top_cases.iterrows(), 1):
        session_info = f" (ID: {case['session_id']})" if 'session_id' in case else ""
        analysis += f"""
{i:2d}. –û–±—Ä–∞–∑–µ—Ü {case['sample_id']}{session_info}
    –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {case['fraud_probability']:.1%}
    –£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {case['risk_level']}
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {'–ú–û–®–ï–ù–ù–ò–ß–ï–°–¢–í–û' if case['prediction'] == 1 else '–õ–µ–≥–∏—Ç–∏–º–Ω–æ'}
"""

    analysis += f"""

–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –î–ï–ô–°–¢–í–ò–Ø–ú:

–ù–ï–ú–ï–î–õ–ï–ù–ù–´–ï –î–ï–ô–°–¢–í–ò–Ø (–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫):
"""

    if results['high_risk_count'] > 0:
        analysis += f"""- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å {results['high_risk_count']} —Å–ª—É—á–∞–µ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞
- –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
- –°–≤—è–∑–∞—Ç—å—Å—è —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π
- –£–≤–µ–¥–æ–º–∏—Ç—å —Å–ª—É–∂–±—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
"""
    else:
        analysis += "- –°–ª—É—á–∞–µ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\n"

    analysis += "\n–ú–û–ù–ò–¢–û–†–ò–ù–ì (–°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫):\n"

    if results['medium_risk_count'] > 0:
        analysis += f"""- –£—Å–∏–ª–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ {results['medium_risk_count']} —Å–ª—É—á–∞–µ–≤ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∏—Å–∫–∞
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã
- –ó–∞–ø—Ä–æ—Å–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
"""
    else:
        analysis += "- –°–ª—É—á–∞–µ–≤ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∏—Å–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\n"

    analysis += f"""
–û–ë–©–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
- –†–µ–≥—É–ª—è—Ä–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
- –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
- –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –ø–æ—Ä–æ–≥–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π

"""

    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    if results['predicted_fraud_rate'] > 0.3:
        analysis += "‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è –¥–æ–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞\n"
        analysis += "   –í–æ–∑–º–æ–∂–Ω–æ, –º–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∞–Ω–æ–º–∞–ª–∏–∏\n\n"

    if results['avg_fraud_probability'] < 0.1:
        analysis += "‚ÑπÔ∏è  –ò–ù–§–û–†–ú–ê–¶–ò–Ø: –ù–∏–∑–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ –≤ –≤—ã–±–æ—Ä–∫–µ\n"
        analysis += "   –≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é\n\n"

    analysis += "=== –ö–û–ù–ï–¶ –ê–ù–ê–õ–ò–ó–ê ==="

    return analysis


def create_summary_files(results: Dict[str, Any], analysis: str,
                        report_df: pd.DataFrame, output_dir: Path):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        analysis: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        report_df: –û—Ç—á–µ—Ç —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    logger.info("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤...")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
    analysis_path = output_dir / "prediction_analysis.txt"
    with open(analysis_path, 'w', encoding='utf-8') as f:
        f.write(analysis)
    logger.info(f"üìÑ –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {analysis_path}")

    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞
    high_risk_cases = report_df[report_df['fraud_probability'] >= 0.7]
    if not high_risk_cases.empty:
        high_risk_path = output_dir / "high_risk_cases.csv"
        high_risk_cases.to_csv(high_risk_path, index=False, encoding='utf-8')
        logger.info(f"üö® –°–ª—É—á–∞–∏ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {high_risk_path}")

    # –°–æ–∑–¥–∞–µ–º JSON —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    import json
    stats_path = output_dir / "prediction_stats.json"
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {stats_path}")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π
        project_dir = Path(__file__).parent.parent
        models_dir = project_dir / "models"
        output_dir = project_dir / "output"

        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        # –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∫–∞–∫ –∞—Ä–≥—É–º–µ–Ω—Ç –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é
        prediction_data_dir = project_dir / "data"

        if len(sys.argv) > 1:
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –∫–∞–∫ –∞—Ä–≥—É–º–µ–Ω—Ç
            prediction_data_dir = Path(sys.argv[1])
            logger.info(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –¥–∞–Ω–Ω—ã–µ –∏–∑: {prediction_data_dir}")

        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        output_dir.mkdir(exist_ok=True)

        logger.info(f"ü§ñ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {models_dir}")
        logger.info(f"üìä –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if not validate_prediction_data(prediction_data_dir):
            return False

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        model_trainer = load_trained_model(models_dir)

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        model_info = model_trainer.get_model_info()
        logger.info(f"‚ÑπÔ∏è  –ú–æ–¥–µ–ª—å: {model_info['model_type']}")
        logger.info(f"‚ÑπÔ∏è  –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {model_info['feature_count']}")
        if 'test_auc' in model_info['metrics']:
            logger.info(f"‚ÑπÔ∏è  Test AUC: {model_info['metrics']['test_auc']:.4f}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        features_processor = RealFeaturesProcessor(str(project_dir / "data"))

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        X, session_ids = prepare_prediction_features(features_processor, str(prediction_data_dir))

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        results = make_predictions(model_trainer, X)

        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        report_df = create_prediction_report(results, X, session_ids, output_dir)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        analysis = analyze_predictions(results, report_df)

        # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        create_summary_files(results, analysis, report_df, output_dir)

        # –í—ã–≤–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info("\n" + "="*60)
        logger.info("‚úÖ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        logger.info("="*60)
        logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {results['total_samples']:,}")
        logger.info(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {results['predicted_fraud_count']:,} ({results['predicted_fraud_rate']:.1%})")
        logger.info(f"üî¥ –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫: {results['high_risk_count']:,}")
        logger.info(f"üü° –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫: {results['medium_risk_count']:,}")
        logger.info(f"üü¢ –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫: {results['low_risk_count']:,}")
        logger.info("="*60)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3 —Å–ª—É—á–∞—è
        logger.info("\nüö® –¢–û–ü-3 –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–• –°–õ–£–ß–ê–ï–í:")
        top_3 = report_df.head(3)
        for i, (_, case) in enumerate(top_3.iterrows(), 1):
            session_info = f" (ID: {case['session_id']})" if 'session_id' in case else ""
            logger.info(f"{i}. –û–±—Ä–∞–∑–µ—Ü {case['sample_id']}{session_info}: {case['fraud_probability']:.1%} —Ä–∏—Å–∫")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        logger.info(f"\nüìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
        logger.info(f"   üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: fraud_predictions_detailed.csv")
        logger.info(f"   üìÑ –ê–Ω–∞–ª–∏–∑: prediction_analysis.txt")
        logger.info(f"   üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: prediction_stats.json")
        if results['high_risk_count'] > 0:
            logger.info(f"   üö® –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫: high_risk_cases.csv")

        logger.info(f"\nüí° –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        if results['high_risk_count'] > 0:
            logger.info(f"1. –°–†–û–ß–ù–û –ø—Ä–æ–≤–µ—Ä—å—Ç–µ {results['high_risk_count']} —Å–ª—É—á–∞–µ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞")
            logger.info(f"2. –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ñ–∞–π–ª: {output_dir}/high_risk_cases.csv")
        logger.info(f"3. –ò–∑—É—á–∏—Ç–µ –∞–Ω–∞–ª–∏–∑: {output_dir}/prediction_analysis.txt")
        logger.info(f"4. –î–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/predict_local_llm.py")

        return True

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
