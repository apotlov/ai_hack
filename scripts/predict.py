#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent.parent / "src"))

import logging
import pandas as pd
import numpy as np
from typing import Dict, Any, List

from feature_extractor import FeatureExtractor
from model_trainer import ModelTrainer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_model(models_dir: Path, model_name: str = "antifraud_model_v1") -> ModelTrainer:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

    Args:
        models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—è–º–∏
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏

    Returns:
        –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π ModelTrainer
    """
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")

    model_trainer = ModelTrainer(str(models_dir))

    try:
        model_trainer.load_model(model_name)
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return model_trainer
    except FileNotFoundError:
        logger.error(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_name}")
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise


def make_predictions(model_trainer: ModelTrainer, X: pd.DataFrame) -> Dict[str, Any]:
    """
    –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

    Args:
        model_trainer: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        X: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    """
    logger.info(f"üîÆ –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤...")

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    predictions = model_trainer.predict(X)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    probabilities = model_trainer.predict_proba(X)
    fraud_probabilities = probabilities[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ"

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    results = {
        'predictions': predictions,
        'fraud_probabilities': fraud_probabilities,
        'total_samples': len(X),
        'predicted_fraud_count': int(predictions.sum()),
        'predicted_fraud_rate': float(predictions.mean()),
        'avg_fraud_probability': float(fraud_probabilities.mean()),
        'max_fraud_probability': float(fraud_probabilities.max()),
        'min_fraud_probability': float(fraud_probabilities.min())
    }

    return results


def create_prediction_report(results: Dict[str, Any], X: pd.DataFrame,
                           output_path: Path) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        X: –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞

    Returns:
        DataFrame —Å –æ—Ç—á–µ—Ç–æ–º
    """
    logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏...")

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    report_df = pd.DataFrame({
        'sample_id': range(len(X)),
        'prediction': results['predictions'],
        'fraud_probability': results['fraud_probabilities'],
        'risk_level': pd.cut(
            results['fraud_probabilities'],
            bins=[0, 0.3, 0.7, 1.0],
            labels=['–ù–∏–∑–∫–∏–π', '–°—Ä–µ–¥–Ω–∏–π', '–í—ã—Å–æ–∫–∏–π']
        )
    })

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫)
    feature_cols = X.columns[:5] if len(X.columns) >= 5 else X.columns
    for col in feature_cols:
        report_df[f'feature_{col}'] = X[col].values

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ (–æ—Ç –≤—ã—Å–æ–∫–æ–π –∫ –Ω–∏–∑–∫–æ–π)
    report_df = report_df.sort_values('fraud_probability', ascending=False)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    report_df.to_csv(output_path, index=False, encoding='utf-8')
    logger.info(f"üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")

    return report_df


def analyze_high_risk_cases(report_df: pd.DataFrame, threshold: float = 0.7) -> None:
    """
    –ê–Ω–∞–ª–∏–∑ —Å–ª—É—á–∞–µ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞

    Args:
        report_df: DataFrame —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞
    """
    high_risk_cases = report_df[report_df['fraud_probability'] >= threshold]

    logger.info(f"üö® –ê–Ω–∞–ª–∏–∑ —Å–ª—É—á–∞–µ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞ (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >= {threshold}):")
    logger.info(f"   –ù–∞–π–¥–µ–Ω–æ —Å–ª—É—á–∞–µ–≤: {len(high_risk_cases)}")

    if len(high_risk_cases) > 0:
        logger.info(f"   –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {high_risk_cases['fraud_probability'].mean():.4f}")
        logger.info(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {high_risk_cases['fraud_probability'].max():.4f}")

        logger.info("   –¢–æ–ø-5 —Å–∞–º—ã—Ö –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤:")
        for idx, row in high_risk_cases.head(5).iterrows():
            logger.info(f"     ID {row['sample_id']}: {row['fraud_probability']:.4f} ({row['risk_level']})")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    logger.info("üéØ –ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏")

    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        data_dir = Path(__file__).parent.parent / "data"
        models_dir = Path(__file__).parent.parent / "models"
        output_dir = Path(__file__).parent.parent / "output"

        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        output_dir.mkdir(exist_ok=True)

        logger.info(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö: {data_dir}")
        logger.info(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {models_dir}")
        logger.info(f"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model_trainer = load_model(models_dir)

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
        model_info = model_trainer.get_model_info()
        logger.info(f"‚ÑπÔ∏è  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
        logger.info(f"   –¢–∏–ø: {model_info['model_type']}")
        logger.info(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {model_info['feature_count']}")
        logger.info(f"   Test AUC: {model_info['metrics'].get('test_auc', 'N/A')}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        logger.info("üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        feature_extractor = FeatureExtractor(str(data_dir))

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        data_summary = feature_extractor.data_loader.get_data_summary()
        logger.info(f"–°–≤–æ–¥–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º: {data_summary}")

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        logger.info("üîß –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")

        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        X_predict = feature_extractor.extract_features()

        if X_predict.empty:
            logger.warning("‚ö†Ô∏è  –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")

            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            X_predict = feature_extractor.create_sample_features(n_samples=50)

            # –£–±–∏—Ä–∞–µ–º user_id –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'user_id' in X_predict.columns:
                user_ids = X_predict['user_id'].copy()
                X_predict = X_predict.drop('user_id', axis=1)
            else:
                user_ids = pd.Series([f'test_user_{i}' for i in range(len(X_predict))])
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º user_id –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'user_id' in X_predict.columns:
                user_ids = X_predict['user_id'].copy()
                X_predict = X_predict.drop('user_id', axis=1)
            else:
                user_ids = pd.Series([f'user_{i}' for i in range(len(X_predict))])

        logger.info(f"üìà –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {X_predict.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X_predict.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        results = make_predictions(model_trainer, X_predict)

        # –í—ã–≤–æ–¥ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        logger.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        logger.info(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {results['total_samples']}")
        logger.info(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ: {results['predicted_fraud_count']}")
        logger.info(f"   –î–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {results['predicted_fraud_rate']:.2%}")
        logger.info(f"   –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {results['avg_fraud_probability']:.4f}")
        logger.info(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {results['max_fraud_probability']:.4f}")

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        output_path = output_dir / "fraud_predictions.csv"
        report_df = create_prediction_report(results, X_predict, output_path)

        # –î–æ–±–∞–≤–ª—è–µ–º user_id –≤ –æ—Ç—á–µ—Ç
        report_df.insert(0, 'user_id', user_ids.values)
        report_df.to_csv(output_path, index=False, encoding='utf-8')

        # –ê–Ω–∞–ª–∏–∑ —Å–ª—É—á–∞–µ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞
        analyze_high_risk_cases(report_df, threshold=0.7)

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        summary_path = output_dir / "prediction_summary.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=== –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∞–Ω—Ç–∏—Ñ—Ä–æ–¥ –º–æ–¥–µ–ª–∏ ===\n\n")
            f.write(f"–î–∞—Ç–∞: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ú–æ–¥–µ–ª—å: {model_info['model_type']}\n")
            f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {model_info['feature_count']}\n\n")

            f.write("=== –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ===\n")
            f.write(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {results['total_samples']}\n")
            f.write(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ: {results['predicted_fraud_count']}\n")
            f.write(f"–î–æ–ª—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {results['predicted_fraud_rate']:.2%}\n")
            f.write(f"–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {results['avg_fraud_probability']:.4f}\n")
            f.write(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {results['max_fraud_probability']:.4f}\n\n")

            f.write("=== –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º —Ä–∏—Å–∫–∞ ===\n")
            risk_counts = report_df['risk_level'].value_counts()
            for risk_level, count in risk_counts.items():
                f.write(f"{risk_level}: {count} ({count/len(report_df):.1%})\n")

            f.write(f"\n=== –¢–æ–ø-10 —Å–∞–º—ã—Ö –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ ===\n")
            top_cases = report_df.head(10)
            for _, row in top_cases.iterrows():
                f.write(f"User {row['user_id']}: {row['fraud_probability']:.4f} ({row['risk_level']})\n")

        logger.info(f"üìÑ –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {summary_path}")

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        recommendations_path = output_dir / "recommendations.txt"
        with open(recommendations_path, 'w', encoding='utf-8') as f:
            f.write("=== –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∞–Ω–∞–ª–∏–∑–∞ ===\n\n")

            high_risk_count = len(report_df[report_df['fraud_probability'] >= 0.7])
            medium_risk_count = len(report_df[
                (report_df['fraud_probability'] >= 0.3) &
                (report_df['fraud_probability'] < 0.7)
            ])

            f.write("1. –ù–ï–ú–ï–î–õ–ï–ù–ù–´–ï –î–ï–ô–°–¢–í–ò–Ø:\n")
            if high_risk_count > 0:
                f.write(f"   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å {high_risk_count} —Å–ª—É—á–∞–µ–≤ —Å –≤—ã—Å–æ–∫–∏–º —Ä–∏—Å–∫–æ–º (‚â•70%)\n")
                f.write("   - –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π\n")
                f.write("   - –°–≤—è–∑–∞—Ç—å—Å—è —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π\n\n")
            else:
                f.write("   - –°–ª—É—á–∞–µ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\n\n")

            f.write("2. –ú–û–ù–ò–¢–û–†–ò–ù–ì:\n")
            if medium_risk_count > 0:
                f.write(f"   - –£—Å–∏–ª–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ {medium_risk_count} —Å–ª—É—á–∞–µ–≤ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∏—Å–∫–∞\n")
                f.write("   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã\n\n")
            else:
                f.write("   - –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥\n\n")

            f.write("3. –ê–ù–ê–õ–ò–ó –ú–û–î–ï–õ–ò:\n")
            if results['avg_fraud_probability'] > 0.5:
                f.write("   - –í—ã—Å–æ–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö\n")
            else:
                f.write("   - –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ –≤ –≤—ã–±–æ—Ä–∫–µ\n")

            f.write(f"   - –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ Test AUC: {model_info['metrics'].get('test_auc', 'N/A')}\n")

        logger.info(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {recommendations_path}")

        logger.info("‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üéØ –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
        logger.info(f"   - –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: {output_path}")
        logger.info(f"   - –°–≤–æ–¥–∫–∞: {summary_path}")
        logger.info(f"   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {recommendations_path}")

        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
